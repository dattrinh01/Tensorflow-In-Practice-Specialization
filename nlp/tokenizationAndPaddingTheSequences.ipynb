{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizationAndPaddingTheSequences.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNSQRNp2GY6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "bad84c57-cbe1-4496-bb5d-4902b6776fca"
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-rc0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/12/8c64cc62149cc21c70c55018502831bbf4d42bd62bed196df7de6830d21b/tensorflow_gpu-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (380.5MB)\n",
            "\u001b[K     |████████████████████████████████| 380.5MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (1.29.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (3.10.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (3.2.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (1.0.8)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-rc0) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-rc0) (47.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc0) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc0) (3.1.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.15.0a20190806 tensorflow-gpu-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFOyQTeZGl-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D2XoRp5HGmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypOpAAVzIV1L",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOWmzg4nHJ4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat'\n",
        "]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCmAQDYRHR4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizerSentences(sentences):\n",
        "    tokenizer = Tokenizer(num_words=1000)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    print(word_index)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAOamf9ZH4iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "773c9620-56df-45f6-b07c-eb355c5bfa79"
      },
      "source": [
        "tokenizerSentences(sentences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrdWpPYYIGJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_sentences = [\n",
        "                 'I love my dog',\n",
        "                 'I love my cat',\n",
        "                 'You love my dog!'\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxoNOCRQINZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74a7ec8c-e56f-4d77-813f-b78c32a9940c"
      },
      "source": [
        "tokenizerSentences(new_sentences)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oIBaReEI6m1",
        "colab_type": "text"
      },
      "source": [
        "# Turning sentences into data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrW0MRIhI9Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmIilA-SWNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "             'You love my dog!',\n",
        "             'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW2jmg7TSsvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizerAndPaddingSequences(sentences, test_sentences):\n",
        "    tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    padded = pad_sequences(sequences, maxlen = 5)\n",
        "\n",
        "    print(\"Word index:\")\n",
        "    print(word_index)\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    print(\"Sequences:\")\n",
        "    print(sequences)\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    print(\"Padding sequences:\")\n",
        "    print(padded)\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "    test_padded = pad_sequences(test_sequences, maxlen = 10)\n",
        "    print(\"Test sequences:\")\n",
        "    print(test_sequences)\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    print(\"Padding test sequences:\")\n",
        "    print(test_padded)\n",
        "    print(\"-------------------------------------------------------------------------\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrYXG7y7UViQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = [\n",
        "                  'I really love my dog',\n",
        "                  'A lot of dogs loves my manatee'\n",
        "]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWqCjPUhUg9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "5c409f2b-77e5-47a7-d850-35a7d53ac56f"
      },
      "source": [
        "tokenizerAndPaddingSequences(sentences, test_sentences)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word index:\n",
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "-------------------------------------------------------------------------\n",
            "Sequences:\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "-------------------------------------------------------------------------\n",
            "Padding sequences:\n",
            "[[ 0  5  3  2  4]\n",
            " [ 0  5  3  2  7]\n",
            " [ 0  6  3  2  4]\n",
            " [ 9  2  4 10 11]]\n",
            "-------------------------------------------------------------------------\n",
            "Test sequences:\n",
            "[[5, 1, 3, 2, 4], [1, 1, 1, 1, 1, 2, 1]]\n",
            "-------------------------------------------------------------------------\n",
            "Padding test sequences:\n",
            "[[0 0 0 0 0 5 1 3 2 4]\n",
            " [0 0 0 1 1 1 1 1 2 1]]\n",
            "-------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}